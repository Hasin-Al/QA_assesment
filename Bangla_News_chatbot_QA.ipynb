{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Load Libraries"
      ],
      "metadata": {
        "id": "JUDFKwIp5veh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIUJTWz55HDO",
        "outputId": "52f3ac42-e987-4440-941a-3a8c25adc629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#installing dependencies\n",
        "!pip install bnlp_toolkit\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z-qpZWk508z",
        "outputId": "dd82e92e-c287-4108-904b-ff1ecb09b3f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bnlp_toolkit\n",
            "  Downloading bnlp_toolkit-4.0.3-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (0.2.0)\n",
            "Collecting gensim==4.3.2 (from bnlp_toolkit)\n",
            "  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (1.26.4)\n",
            "Collecting scipy==1.10.1 (from bnlp_toolkit)\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sklearn-crfsuite==0.3.6 (from bnlp_toolkit)\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting tqdm==4.66.3 (from bnlp_toolkit)\n",
            "  Downloading tqdm-4.66.3-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy==6.2.0 (from bnlp_toolkit)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting emoji==1.7.0 (from bnlp_toolkit)\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (2.32.3)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy==6.2.0->bnlp_toolkit) (0.2.13)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2->bnlp_toolkit) (7.0.5)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite==0.3.6->bnlp_toolkit)\n",
            "  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite==0.3.6->bnlp_toolkit) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite==0.3.6->bnlp_toolkit) (0.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp_toolkit) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp_toolkit) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp_toolkit) (2024.9.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp_toolkit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp_toolkit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp_toolkit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp_toolkit) (2024.8.30)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim==4.3.2->bnlp_toolkit) (1.16.0)\n",
            "Downloading bnlp_toolkit-4.0.3-py3-none-any.whl (22 kB)\n",
            "Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Downloading tqdm-4.66.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=12229b2f36207b80fb1a6f9e8ca8651ed340c9d9e2a3b07ca65566597288baff\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, tqdm, scipy, python-crfsuite, ftfy, sklearn-crfsuite, gensim, bnlp_toolkit\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.6\n",
            "    Uninstalling tqdm-4.66.6:\n",
            "      Successfully uninstalled tqdm-4.66.6\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "Successfully installed bnlp_toolkit-4.0.3 emoji-1.7.0 ftfy-6.2.0 gensim-4.3.2 python-crfsuite-0.9.11 scipy-1.10.1 sklearn-crfsuite-0.3.6 tqdm-4.66.3\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imporing libraris\n",
        " #importing all necessari libraries . For tokenization we will use bnlp tokenizer and create our own embedder\n",
        "from bnlp import NLTKTokenizer\n",
        "import string\n",
        "from bnlp import CleanText\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import faiss\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fwEN3pH56sJ",
        "outputId": "fe88f251-23ae-4727-862d-6d6ace5a54b2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text cleaning and Preparing our dataset for Embedder and Creating our own vocabulary\n"
      ],
      "metadata": {
        "id": "Z9PJaKEE8J-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading our dataset\n",
        "#We will load two dataframe . Archieve for archived data and current date data for todays news\n",
        "\n",
        "archive = pd.read_csv('/content/archive.csv')\n",
        "current_date = pd.read_csv('/content/current_date.csv')"
      ],
      "metadata": {
        "id": "9EwLf1DD5_Tv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This cell , we will load our tokenizer . we will use BNLP toolkit .\n",
        "Also  define some necessary functions like text cleaning and preprocessing for our task\n",
        "'''\n",
        "\n",
        "tokenizer = NLTKTokenizer()\n",
        "clean_text = CleanText(\n",
        "   fix_unicode=True,\n",
        "   unicode_norm=True,\n",
        "   unicode_norm_form=\"NFKC\",\n",
        "   remove_url=False,\n",
        "   remove_email=True,\n",
        "   remove_emoji=True,\n",
        "   remove_number=True,\n",
        "   remove_digits=True,\n",
        "   remove_punct=True,\n",
        "   replace_with_url=\"<URL>\",\n",
        "   replace_with_email=\"<EMAIL>\",\n",
        "   replace_with_number=\"<NUMBER>\",\n",
        "   replace_with_digit=\"<DIGIT>\",\n",
        "   replace_with_punct = \"\"\n",
        ")\n",
        "\n",
        "def remove_hyphens(text):\n",
        "    # Remove hyphens from the text\n",
        "    cleaned_text = text.replace('\\u002D', '')\n",
        "    return cleaned_text\n",
        "\n",
        "def remove_unwanted_char(text):\n",
        "  cleaned_text = text.replace('—','')\n",
        "  cleaned_text = cleaned_text.replace('<','')\n",
        "  cleaned_text = cleaned_text.replace('>','')\n",
        "  cleaned_text = cleaned_text.replace('/','')\n",
        "  cleaned_text = cleaned_text.replace('...','')\n",
        "\n",
        "  return cleaned_text\n",
        "\n",
        "# Chunk text function remains the same\n",
        "def chunk_text(text, chunk_size=100):\n",
        "    words = text.split()  # Split the text into words\n",
        "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "def remove_parentheses_and_text(lines):\n",
        "    cleaned_lines = [re.sub(r'\\([^)]*\\)', '', line) for line in lines]\n",
        "    return cleaned_lines\n",
        "\n",
        "def is_bangla(text):\n",
        "    bangla_pattern = re.compile(r'^[\\u0980-\\u09FF\\s]+$')\n",
        "    return bool(bangla_pattern.match(text))\n",
        "\n",
        "def preprocess(text):\n",
        "  text = clean_text(text)\n",
        "  text = remove_hyphens(text)\n",
        "  text = remove_unwanted_char(text)\n",
        "  text = remove_parentheses_and_text(text)\n",
        "  #text = is_bangla(text)\n",
        "\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "7CbDZ9Yy62ZD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now preprocess and make a vocab\n",
        "#we will create our own embedder using word2vec\n",
        "#dot products between words will extract sementic meaning as we will not use sentence embedder for this case\n",
        "\n",
        "archive_list = archive['news_text'].apply(lambda text: [sent.strip() for sent in text.split('।') + text.split('?') + text.split('!') if sent.strip()])\n",
        "current_list = current_date['text'].apply(lambda text: [sent.strip() for sent in text.split('।') + text.split('?') + text.split('!') if sent.strip()])"
      ],
      "metadata": {
        "id": "yYlSFiCO7M9g"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making our dataseet by joining archive and current news to train our embedder\n",
        "\n",
        "archive_sentence = [sentence for sublist in archive_list for sentence in sublist]\n",
        "current_sentence = [sentence for sublist in current_list for sentence in sublist]\n",
        "\n",
        "\n",
        "main_list = archive_sentence + current_sentence\n"
      ],
      "metadata": {
        "id": "9yGUIBkY7xJ5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning text for create our vocab\n",
        "#clena unwanted words or chars\n",
        "texts = []\n",
        "\n",
        "for line in main_list:\n",
        "  temp = clean_text(line)\n",
        "  temp = remove_hyphens(temp)\n",
        "  temp = remove_unwanted_char(temp)\n",
        "  texts.append(temp)\n",
        "\n",
        "texts = [text.replace('\\n', ' ') for text in texts]\n"
      ],
      "metadata": {
        "id": "DebHHZ4y8Hxo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing non bengali words\n",
        "print(len(texts))\n",
        "texts = [text for text in texts if is_bangla(text)]\n",
        "print(len(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKQvp7p49G1T",
        "outputId": "2cc426af-103f-4ac8-e9fd-e47325711a39"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15818\n",
            "12088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenized text to train our embedder\n",
        "tokenized_text = []\n",
        "\n",
        "for text in texts:\n",
        "  token = tokenizer.word_tokenize(text)\n",
        "  tokenized_text.append(token)"
      ],
      "metadata": {
        "id": "4DXUwMdU9xnR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading my own vocabulary, i have created earlier for my later use\n",
        "#to increase our benglai word range\n",
        "with open('/content/drive/MyDrive/dschatbot/vocab.pkl','rb') as f:\n",
        "  vocab = pickle.load(f)\n",
        "\n",
        "print(f\"the length of vacab is {len(vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoYScZOg9y8g",
        "outputId": "583d6059-737a-4316-cf56-5a3b224f1bd9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the length of vacab is 535477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking duplicate words with our loaded vocab and add unique words\n",
        "flattened_tokens = [word for sublist in tokenized_text for word in sublist]\n",
        "tokenized_set = set(flattened_tokens)\n",
        "vocab_set = set(vocab.keys())\n",
        "\n",
        "duplicates_word = set.intersection(vocab_set,tokenized_set)\n",
        "print(len(duplicates_word))\n",
        "\n",
        "new_words = set.difference(tokenized_set,vocab_set)\n",
        "print(len(new_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkDxVybF-AXh",
        "outputId": "60612fa1-b6eb-48a5-a91b-02ec7a16bcba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15664\n",
            "1075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove duplicates from our newly words\n",
        "\n",
        "unique_list = list(set(new_words))\n",
        "len(unique_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XODNUupm-j9i",
        "outputId": "a3e2e5a0-7449-4a2d-a676-84fce1a31a2f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1075"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#adding new unique words to our vocab\n",
        "current_vocab_size = len(vocab)\n",
        "\n",
        "for i,word in enumerate(unique_list):\n",
        "  vocab[word] = current_vocab_size + i\n",
        "\n",
        "print(f'old vocab size {current_vocab_size}')\n",
        "print(f\"new vocab size {len(vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71T2l4Xv-waW",
        "outputId": "de9e8e62-75a7-4fef-ffba-4c85cffb8893"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "old vocab size 535477\n",
            "new vocab size 536552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#optional\n",
        "#saving our new vocab for later use\n",
        "#so when you use you can just load this vocab which will reduce whole process till now\n",
        "\n",
        "\n",
        "with open('/content/vocab.pkl','wb') as f:\n",
        "  pickle.dump(vocab,f)\n"
      ],
      "metadata": {
        "id": "skblzXfS-6-p"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load our pretrained word embedder\n",
        "#Trained with bengali words to my later use\n",
        "\n",
        "\n",
        "\n",
        "model = Word2Vec.load('/content/drive/MyDrive/dschatbot/word2vec_model.bin')\n",
        "model.build_vocab(tokenized_text, update=True)\n",
        "model.train(tokenized_text, total_examples=len(tokenized_text), epochs=model.epochs)\n",
        "\n",
        "#save our embedder for later use\n",
        "model.save('word2vec_model.bin')\n",
        "print(f\"Vocabulary size after update: {len(model.wv.index_to_key)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E7shCuz_FPx",
        "outputId": "5f31ec07-246f-4281-a9ba-0707c04c0578"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size after update: 536552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#optional\n",
        "#check my embedder's performance\n",
        "#fucntion to find similar words\n",
        "def find_similar_words(word, topn=5):\n",
        "    similar_words = model.wv.similar_by_word(word, topn=topn)\n",
        "    return similar_words"
      ],
      "metadata": {
        "id": "svAh4iX__mB5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optional\n",
        "#you are requested to check with any bengali words and find out how good my embedder is\n",
        "\n",
        "similar = find_similar_words('খেলা', topn=10)\n",
        "similar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7Z740XM_9jZ",
        "outputId": "404adbb3-eb47-46cd-a316-34ed0268ccf5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('খেলাটা', 0.7625560164451599),\n",
              " ('খেলাও', 0.7287689447402954),\n",
              " ('ম্যাচটি', 0.7021195292472839),\n",
              " ('খেলাই', 0.6814424395561218),\n",
              " ('ম্যাচ', 0.6699913144111633),\n",
              " ('লড়াইটা', 0.6571086049079895),\n",
              " ('খেলাটি', 0.652617871761322),\n",
              " ('টুর্নামেন্ট', 0.6470634341239929),\n",
              " ('ম্যাচটা', 0.6422958374023438),\n",
              " ('খেলাটাও', 0.639151930809021)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess data for Indexing"
      ],
      "metadata": {
        "id": "WT1bZgh0BKMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to filter and return only Bangla words from a list of words\n",
        "# you might wonder why this?\n",
        "# our previous functions could not perform in dataframe\n",
        "# as my target to clean data in dataframe saved in column to reduce my time if i will work later on this projet\n",
        "# hence , i re wrote the function as per my requirements\n",
        "\n",
        "def is_bangla_word(line):\n",
        "    \"\"\"\n",
        "    Takes a list of words and returns a list containing only the words that have Bangla characters.\n",
        "\n",
        "    Args:\n",
        "        word_list (list): A list of words (strings).\n",
        "\n",
        "    Returns:\n",
        "        list: A list containing only words that are in Bangla.\n",
        "    \"\"\"\n",
        "    bangla_pattern = re.compile(r'^[\\u0980-\\u09FF]+$')  # Pattern to match only Bangla characters\n",
        "\n",
        "    word_list = tokenizer.word_tokenize(line)\n",
        "\n",
        "    # Filter and return words that match the Bangla pattern\n",
        "    return [word for word in word_list if bangla_pattern.fullmatch(word)]\n",
        "\n"
      ],
      "metadata": {
        "id": "ilYL72DRAIFB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning function for dataframe too\n",
        "\n",
        "def full_clean(line):\n",
        "\n",
        "\n",
        "  texts = []\n",
        "\n",
        "\n",
        "  temp = clean_text(line)\n",
        "  #print(temp)\n",
        "  #print('-------')\n",
        "  temp = remove_hyphens(temp)\n",
        "  #print(temp)\n",
        "  #print('---')\n",
        "  temp = remove_unwanted_char(temp)\n",
        "  #print(temp)\n",
        "  #print('--------')\n",
        "  temp = is_bangla_word(temp)\n",
        "  #print(temp)\n",
        "  #print(\"--------\")\n",
        "  temp = clean_text(temp)\n",
        "  texts.append(temp)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return texts\n",
        "\n",
        "def main_clean(text):\n",
        "  line = tokenizer.word_tokenize(text)\n",
        "  #print(line)\n",
        "  line = full_clean(line)\n",
        "  return line\n"
      ],
      "metadata": {
        "id": "so-FtB5HA35T"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning category , title and text and add new column into the dataframe\n",
        "#Remember we will create separate index for archive and current date to save time to interact with user\n",
        "\n",
        "archive['clean_title'] = archive['title'].apply(main_clean)\n",
        "archive['clean_cat'] = archive['category'].apply(main_clean)\n",
        "archive['clean_text'] = archive['news_text'].apply(main_clean)\n",
        "current_date['clean_title'] = current_date['title'].apply(main_clean)\n",
        "current_date['clean_text'] = current_date['text'].apply(main_clean)"
      ],
      "metadata": {
        "id": "Y5-gKpDOBR35"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optional\n",
        "# save the cleaned dataframe for later use\n",
        "\n",
        "archive.to_csv('with_clean_text_archive.csv')\n",
        "current_date.to_csv('with_clean_text_current.csv')"
      ],
      "metadata": {
        "id": "rsyDlnNiBimM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chunking function\n",
        "# i will use chunking this time for text only as news text can be a bit longer\n",
        "# for creating index . News text will be chunked and then will be indexed using FAiSS maybe\n",
        "\n",
        "def chunk_list(word_list):\n",
        "    word_list = ' '.join(word_list)\n",
        "    word_list = word_list.split()\n",
        "    chunk_size = 30\n",
        "    return [word_list[i:i + chunk_size] for i in range(0, len(word_list), chunk_size)]"
      ],
      "metadata": {
        "id": "f7W9K6XuBvw8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chunking our text for archive and current date data frame\n",
        "\n",
        "archive['chunked_text'] = archive['clean_text'].apply(lambda x : chunk_list(x))\n",
        "current_date['chunked_text'] = current_date['clean_text'].apply(lambda x : chunk_list(x))"
      ],
      "metadata": {
        "id": "6ew2tljMBwAT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Here in this cell, we will use our own techniques to extract semantics.\n",
        "As i have chunked all the text and did not use any pretrained popular embedder\n",
        "i will calculate words tf-idf score for each chunk and then this tf-idf scores will be multiplied each\n",
        "words embedding to figure words importance in semantics . tf-idf score is popular method to find important words . so\n",
        "i will try this technique to find our semantics of chunks . Here , dot production of chunks embedding could be more helpful .\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "def compute_tfidf_weights(chunk):\n",
        "\n",
        "    vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(), lowercase=False)\n",
        "    tfidf_matrix = vectorizer.fit_transform([' '.join(chunk)])\n",
        "    tfidf_scores = dict(zip(vectorizer.get_feature_names_out(), tfidf_matrix.toarray()[0]))\n",
        "\n",
        "    return tfidf_scores\n",
        "\n",
        "\n",
        "def embed_text_chunk(text_chunk, word2vec_model):\n",
        "\n",
        "    tfidf_scores = compute_tfidf_weights(text_chunk)\n",
        "    embeddings = []\n",
        "    for word in text_chunk:\n",
        "        if word in word2vec_model.wv:\n",
        "            tfidf_weight = tfidf_scores.get(word, 1.0)  # Default weight is 1.0 if word not in TF-IDF scores\n",
        "            weighted_embedding = word2vec_model.wv[word] * tfidf_weight  # Multiply embedding by TF-IDF weight\n",
        "            embeddings.append(weighted_embedding)  # Store the weighted embedding\n",
        "\n",
        "\n",
        "    if embeddings:\n",
        "        final_embedding = np.zeros(word2vec_model.vector_size)\n",
        "\n",
        "\n",
        "        for emb in embeddings:\n",
        "            final_embedding += emb  # This can be changed to np.dot if you want a different combination logic\n",
        "\n",
        "        return final_embedding\n",
        "    else:\n",
        "        return np.zeros(word2vec_model.vector_size)\n",
        "\n",
        "\n",
        "def embed_text_chunks(text_chunks, word2vec_model):\n",
        "\n",
        "    chunk_embeddings = [embed_text_chunk(chunk, word2vec_model) for chunk in text_chunks]\n",
        "\n",
        "\n",
        "    if chunk_embeddings:\n",
        "        chunk_weights = [np.linalg.norm(embedding) for embedding in chunk_embeddings]\n",
        "        total_weight = sum(chunk_weights)\n",
        "\n",
        "\n",
        "        final_embedding = np.zeros(word2vec_model.vector_size)\n",
        "\n",
        "        for embedding, weight in zip(chunk_embeddings, chunk_weights):\n",
        "            if total_weight > 0:\n",
        "                final_embedding += embedding * (weight / total_weight)\n",
        "\n",
        "        return final_embedding if total_weight > 0 else np.zeros(word2vec_model.vector_size)\n",
        "    else:\n",
        "        return np.zeros(word2vec_model.vector_size)\n"
      ],
      "metadata": {
        "id": "_SHGN-pQCSfq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get text embeddings for both the dataframe\n",
        "\n",
        "archive['text_embedding'] = archive['chunked_text'].apply(lambda x: embed_text_chunks(x, model))\n",
        "current_date['text_embedding'] = current_date['chunked_text'].apply(lambda x: embed_text_chunks(x, model))\n"
      ],
      "metadata": {
        "id": "bIlXMSO3CUix"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to embedding category\n",
        "\n",
        "def get_cat_embeddings(tokens, model):\n",
        "    tokens = ' '.join(tokens)\n",
        "    tokens = tokens.split()\n",
        "    tfidf_scores = compute_tfidf_weights(tokens)\n",
        "    embeddings = []\n",
        "    weights = []\n",
        "\n",
        "    for word in tokens:\n",
        "        if word in model.wv:\n",
        "            tfidf_weight = tfidf_scores.get(word, 1.0)\n",
        "            embeddings.append(model.wv[word] * tfidf_weight)\n",
        "            weights.append(tfidf_weight)\n",
        "\n",
        "\n",
        "    if embeddings:\n",
        "        total_weight = sum(weights)\n",
        "        weighted_average_embedding = np.sum(embeddings, axis=0) / total_weight\n",
        "        return weighted_average_embedding if total_weight > 0 else np.zeros(model.vector_size)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n"
      ],
      "metadata": {
        "id": "YpriqWt8FVjr"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# title embedding\n",
        "\n",
        "def get_title_embeddings(tokens, model):\n",
        "\n",
        "\n",
        "    tokens = ' '.join(tokens).split()\n",
        "    tfidf_scores = compute_tfidf_weights(tokens)\n",
        "    embeddings = []\n",
        "    weights = []\n",
        "\n",
        "    for word in tokens:\n",
        "        if word in model.wv:\n",
        "            tfidf_weight = tfidf_scores.get(word, 1.0)\n",
        "            weighted_embedding = model.wv[word] * tfidf_weight\n",
        "            embeddings.append(weighted_embedding)\n",
        "            weights.append(tfidf_weight)\n",
        "\n",
        "\n",
        "    if embeddings:\n",
        "        final_embedding = np.zeros(model.vector_size)\n",
        "        total_weight = sum(weights)\n",
        "        for emb, weight in zip(embeddings, weights):\n",
        "            final_embedding += emb\n",
        "\n",
        "        if total_weight > 0:\n",
        "            final_embedding /= total_weight\n",
        "\n",
        "        return final_embedding\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n"
      ],
      "metadata": {
        "id": "xXZlQfURFnlT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# category and title embedding for archive news\n",
        "# title embedding for current news\n",
        "\n",
        "archive['title_embedding'] = archive['clean_title'].apply(lambda x : get_title_embeddings(x,model))\n",
        "current_date['title_embedding'] = current_date['clean_title'].apply(lambda x: get_title_embeddings(x,model))\n",
        "archive['cat_embdding'] = archive['clean_cat'].apply(lambda x: get_cat_embeddings(x,model))\n"
      ],
      "metadata": {
        "id": "_93lp0FMF6nw"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating FAISS indexs\n"
      ],
      "metadata": {
        "id": "Ni3TSLGyGfW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and save our faiss indexes for archive's category , title and text\n",
        "# This system will search different indexes based on users query\n",
        "#\n",
        "\n",
        "\n",
        "embedding_dim = archive['title_embedding'].iloc[0].shape[0]\n",
        "\n",
        "\n",
        "a_title_index = faiss.IndexFlatL2(embedding_dim)\n",
        "a_text_index = faiss.IndexFlatL2(embedding_dim)\n",
        "a_category_index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "# Convert embeddings to a NumPy array and add them to the respective FAISS index\n",
        "a_title_embeddings = np.vstack(archive['title_embedding'].values)\n",
        "a_text_embeddings = np.vstack(archive['text_embedding'].values)\n",
        "a_category_embeddings = np.vstack(archive['cat_embdding'].values)\n",
        "\n",
        "a_title_index.add(a_title_embeddings)\n",
        "a_text_index.add(a_text_embeddings)\n",
        "a_category_index.add(a_category_embeddings)\n",
        "\n",
        "# save the indexes for later use\n",
        "# optional\n",
        "# Save the indices to disk\n",
        "faiss.write_index(a_title_index, 'a_title_index.index')\n",
        "faiss.write_index(a_text_index, 'a_text_index.index')\n",
        "faiss.write_index(a_category_index, 'a_category_index.index')\n"
      ],
      "metadata": {
        "id": "g_v25GjBGK1z"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#similar task for current news\n",
        "# if user serach for current news, system will use current indexs to interact\n",
        "\n",
        "\n",
        "c_embedding_dim = current_date['title_embedding'].iloc[0].shape[0]\n",
        "\n",
        "\n",
        "c_title_index = faiss.IndexFlatL2(c_embedding_dim)\n",
        "c_text_index = faiss.IndexFlatL2(c_embedding_dim)\n",
        "\n",
        "\n",
        "c_title_embeddings = np.vstack(current_date['title_embedding'].values)\n",
        "c_text_embeddings = np.vstack(current_date['text_embedding'].values)\n",
        "\n",
        "c_title_index.add(c_title_embeddings)\n",
        "c_text_index.add(c_text_embeddings)\n",
        "\n",
        "\n",
        "#optional\n",
        "faiss.write_index(c_title_index, 'c_title_index.index')\n",
        "faiss.write_index(c_text_index, 'c_text_index.index')"
      ],
      "metadata": {
        "id": "ZepvWCZnGeRY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This function will be designed to retrive information based on users query\n",
        "for details , read documentation\n",
        "\n",
        "Once we can match index information , we wil retrive index and based on the index\n",
        "we will return all the other information from archive dataframe\n",
        "\n",
        "'''\n",
        "\n",
        "def search_archived_faiss(query_embedding, index_type, k=5):\n",
        "\n",
        "    if index_type == 'title':\n",
        "        distances, indices = a_title_index.search(np.array([query_embedding]), k)\n",
        "    elif index_type == 'text':\n",
        "        distances, indices = a_text_index.search(np.array([query_embedding]), k)\n",
        "    elif index_type == 'category':\n",
        "        distances, indices = a_category_index.search(np.array([query_embedding]), k)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid index_type. Must be one of: 'title', 'text', 'category'.\")\n",
        "\n",
        "\n",
        "    #print(f\"distance is {distances}\")\n",
        "    # Retrieve the corresponding rows from the DataFrame based on the indices returned\n",
        "    result_data = archive.iloc[indices[0]]\n",
        "\n",
        "    # Return the relevant data (e.g., news link, title, text, etc.)\n",
        "    results = []\n",
        "    for idx, row in result_data.iterrows():\n",
        "        results.append({\n",
        "            'title': row['title'],\n",
        "            'category': row['category'],\n",
        "            'news_link': row['link'],\n",
        "            'news_text': row['news_text']\n",
        "        })\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "85gvL1f1Gecu"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#similar task as we do for archive indexes\n",
        "\n",
        "def search_current_faiss(query_embedding, index_type, k=5):\n",
        "\n",
        "    if index_type == 'title':\n",
        "        distances, indices = c_title_index.search(np.array([query_embedding]), k)\n",
        "    elif index_type == 'text':\n",
        "        distances, indices = c_text_index.search(np.array([query_embedding]), k)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid index_type. Must be one of: 'title', 'text', 'category'.\")\n",
        "\n",
        "\n",
        "    result_data = current_date.iloc[indices[0]]\n",
        "\n",
        "    print(f\"distance is {distances}\")\n",
        "\n",
        "\n",
        "    results = []\n",
        "    for idx, row in result_data.iterrows():\n",
        "        results.append({\n",
        "            'title': row['title'],\n",
        "            'news_link': row['url'],\n",
        "            'news_text': row['text']\n",
        "        })\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "hlwtdmbQIVm7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator funcitons"
      ],
      "metadata": {
        "id": "xDccscKKIl32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_bengali_query(query):\n",
        "    \"\"\"\n",
        "    This function will find out user's promt related to current date news or not\n",
        "    The idea is if current date , we will search current date's index\n",
        "    Else we will search archive index\n",
        "    \"\"\"\n",
        "\n",
        "    current_indicators = ['আজ', 'আজকের', 'আজকে']\n",
        "\n",
        "\n",
        "    for word in current_indicators:\n",
        "        if word in query:\n",
        "            return 'current'\n",
        "\n",
        "    return 'archive'"
      ],
      "metadata": {
        "id": "E0_WEAw4IkNT"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_cat(query):\n",
        "  '''\n",
        "  This functionality will check if the user's prompt is related to title or category or text\n",
        "  and then retrive the information accordingly to the required faiss index\n",
        "  '''\n",
        "\n",
        "  query = tokenizer.word_tokenize(query)\n",
        "\n",
        "  if (len(query) <=2 ):\n",
        "    return 'category'\n",
        "  elif (len(query ) >2 and len(query) <=13):\n",
        "    return 'title'\n",
        "\n",
        "  else:\n",
        "    return 'text'"
      ],
      "metadata": {
        "id": "JJoz-29mIk_v"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(query,idx):\n",
        "\n",
        "  '''\n",
        "  This function will process user's prompt\n",
        "  if the prompt is text/title/category, we will embedded the input accordingly\n",
        "  '''\n",
        "\n",
        "  embeddings = []\n",
        "\n",
        "  #print(query)\n",
        "  query = query\n",
        "  word2vec_model = model\n",
        "\n",
        "  if idx == 'title':\n",
        "\n",
        "    #print(query)\n",
        "    query = query\n",
        "    #print(query)\n",
        "    avg_embedding = get_title_embeddings(query,word2vec_model)\n",
        "\n",
        "  elif idx == 'category':\n",
        "    avg_embedding = get_cat_embeddings(query,word2vec_model)\n",
        "\n",
        "  elif idx == 'text':\n",
        "    qry = ' '.join(query)\n",
        "    #print(qry)\n",
        "    qry = qry.split()\n",
        "    #print(qry)\n",
        "\n",
        "    chunks = chunk_list(qry)\n",
        "\n",
        "    #print(chunks)\n",
        "\n",
        "    avg_embedding = embed_text_chunks(chunks, word2vec_model)\n",
        "\n",
        "  else:\n",
        "    print(f' Something wrong happend . Try again with appropriate words or text. Thank You')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return query, avg_embedding\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MCxFfrkaKSDV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrive_info(query):\n",
        "  '''\n",
        "  This would be the main function to retrive informations\n",
        "  It will check the query is a current / archive one\n",
        "  It will chekc the query is a text/title/categroy\n",
        "\n",
        "  and then retrive the information accordingly\n",
        "\n",
        "  '''\n",
        "  query = tokenizer.word_tokenize(query)\n",
        "  query = full_clean(query)\n",
        "\n",
        "  #query,embedding = process_query(query)\n",
        "  #print(f\" embedding is {embedding}\")\n",
        "\n",
        "  st_qry = ' '.join(query)\n",
        "  #print(st_qry)\n",
        "\n",
        "\n",
        "  status = check_bengali_query(st_qry)\n",
        "  #print(status)\n",
        "  idx = check_cat(st_qry)\n",
        "  #print(idx)\n",
        "\n",
        "  query,embedding = process_query(query,idx)\n",
        "\n",
        "  if status == 'current':\n",
        "    if idx == 'category':\n",
        "      print(f\"please type more than two keywoords . Thank you\"\n",
        "      )\n",
        "    else:\n",
        "      result = search_current_faiss(embedding , idx, k=5)\n",
        "\n",
        "  elif status == 'archive':\n",
        "    result = search_archived_faiss(embedding, idx, k=5)\n",
        "\n",
        "  else:\n",
        "    print(f'Sorry i do not understand . Would you be more specific please?')\n",
        "\n",
        "  return result\n",
        "\n"
      ],
      "metadata": {
        "id": "-lTpCBgOKzTB"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator(query):\n",
        "  '''\n",
        "  Till now this is the main generator function which will be replaced with a LLM like bert or T5.\n",
        "  Generator function will use a text summarizer which i trained on a different notebook, a bert architecture fine tunned with my\n",
        "  dataset . And will reply the summarized text using all the text it will retrive .\n",
        "  '''\n",
        "  result = retrive_info(query)\n",
        "\n",
        " # print(type(result))\n",
        "  #print(result)\n",
        "\n",
        "  print(\"--------------------------------------------------\")\n",
        "  print(\"Dear User,\")\n",
        "  print(\"Thank you for your query. I have successfully retrieved the following news articles related to your search:\")\n",
        "  print()\n",
        "\n",
        "    # Ensure each result appears on a new line\n",
        "  for item in result:\n",
        "      print(f\"- {item}\")  # You can format it as desired (e.g., adding bullets or numbers)\n",
        "\n",
        "  print()\n",
        "  print(\"If you need any further assistance or more information, please feel free to ask.\")\n",
        "  print(\"--------------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "bVV0x13JLNRc"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatbot interface\n",
        "def chatbot():\n",
        "    print(\"Chatbot: Hi! Ask me anything, or type 'exit' to quit.\\n\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "        try:\n",
        "            answer = generator(user_input)\n",
        "            print(f\"Chatbot: {answer}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Chatbot: Sorry, I encountered an error: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "yDY2wcpmL0p1"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqw5kIZ8MVHa",
        "outputId": "94d3008f-1d19-46e0-edb0-15afdcd03f76"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: Hi! Ask me anything, or type 'exit' to quit.\n",
            "\n",
            "You: আজকের রাজনীতির খবর কি?\n",
            "distance is [[250.94629 277.79846 313.7649  313.9292  323.8869 ]]\n",
            "--------------------------------------------------\n",
            "Dear User,\n",
            "Thank you for your query. I have successfully retrieved the following news articles related to your search:\n",
            "\n",
            "- {'title': 'টমেটোর এই পুষ্টিগুণগুলো জানেন কি', 'news_link': 'https://bangla.thedailystar.net/life-living/food-recipe/news-634096', 'news_text': \"টমেটো একটি জনপ্রিয় সবজি, যা সারা বিশ্বে বিভিন্ন রকম রান্না এবং সালাদে ব্যবহৃত হয়।অনেকে কাঁচা টমেটো খেতে পছন্দ করেন। টমেটো স্বাদে ও পুষ্টিগুণে অনন্য। বর্তমানে এদেশে সারাবছর টমেটো পাওয়া গেলেও শীতকালে এটির ফলন বেশী হয়। তাই টমেটো মোসুমি সবজি হিসেবে বিবেচিত হয়। আর যেকোনো মৌসুমি সবজি শরীরের জন্য ভীষণ উপকারী। চলুন জেনে নিই টমেটোর পুষ্টিগুণ। জানিয়েছেন এমএইচ শমরিতা মেডিকেল কলেজ অ্যান্ড হাসপাতালের পুষ্টিবিদআঞ্জুমান আরা শিমুল। তিনি বলেন, টমেটো তাজা, রসালো, মিষ্টি এবং সামান্য টক জাতীয় ফল যেটি সাধারণত সবজি হিসেবেই বেশি পরিচিত। টমেটো বিভিন্ন ধরনের খাবারে ব্যবহৃত হয়। এটি সালাদ, স্যুপ, রান্না করা খাবার, জুস, সস এবং পিউরির মতো বিভিন্ন রূপে ব্যবহার করা যায়। রান্না করা টমেটোতে লাইকোপিনের শোষণ ক্ষমতা বৃদ্ধি পায়, যা শরীরের জন্য আরও উপকারী। টমেটোর পুষ্টি উপাদানটমেটোতে ভিটামিন এবং খনিজ উপাদান রয়েছে যা স্বাস্থ্যকর। একটি বড় টমেটো (প্রায় ১৮২ গ্রাম) থেকে পাওয়া যায় -পানি- প্রায় ৯৫%শক্তি - ৩২.৮ কিলোক্যালরিপ্রোটিন - ১.৬ গ্রামচর্বি - ০.৩৬৪ গ্রামকার্বোহাইড্রেট - ৭.০৮গ্রামফাইবার - ২.১৮ গ্রামচিনি - ৪.৭৯ গ্রামগ্লুকোজ - ২.২৮ গ্রামফ্রুক্টোজ - ২.৪৮ গ্রামক্যালসিয়াম - ১৮.২ মিলিগ্রামআয়রন - ০.৪৯২ মিলিগ্রামম্যাগনেসিয়াম - ২০ মিলিগ্রামফসফরাস - ৪৩.৭ মিলিগ্রামপটাসিয়াম - ৪৩১ মিলিগ্রামসোডিয়াম - ৯.১ মিলিগ্রামদস্তা - ০.৩০৯ মিলিগ্রামতামা - ০.১০৭ মিলিগ্রামভিটামিন এ - ৪২ মাইক্রোগ্রামভিটামিন কে - ৭.৯ মাইক্রোগ্রামটমেটোর উপকারিতাটমেটোর স্বাস্থ্য উপকারিতা অনেক। প্রতিদিনের খাদ্যতালিকায় টমেটো অন্তর্ভুক্ত করলে রক্তের ট্রাইগ্লিসারাইডের মাত্রা হ্রাস পায়। টমেটো পোস্টপ্র্যান্ডিয়াল লিপেমিয়া প্রতিরোধে সহায়তা করতে পারে এবং এটি করার ফলে এথেরোস্ক্লেরোসিস হওয়ার ঝুঁকি হ্রাস পায়। এ ছাড়া নিয়মিত টমেটো খেলে এটি ওজন কমাতে সাহায্য করে।আরওতেঁতুল খেলে কি সত্যিই রক্ত পানি হয়ে যায়?অক্সিডেটিভ স্ট্রেস এবং প্রদাহ হ্রাসটমেটো লাইকোপিন, বিটা-ক্যারোটিন, লুটেইন, ক্লোরোজেনিক অ্যাসিড এবং নারিনজেনিন সহ বিভিন্ন অ্যান্টিঅক্সিডেন্টে পূর্ণ। অ্যান্টিঅক্সিডেন্টগুলি আমাদের শরীরের অক্সিডেটিভ স্ট্রেসের ফলে সৃষ্ট সমস্যা ঠিক করতে সাহায্য করে। দূষণ থেকে শুরু করে অতিবেগুনি রশ্মি অনেক কিছু এই অক্সিডেটিভ স্ট্রেসের কারণ হতে পারে। টমেটোতে লুটেইনও রয়েছে, যা বয়স-সম্পর্কিত ম্যাকুলার অবক্ষয় রোধ করতে সাহায্য করে। ক্লোরোজেনিক অ্যাসিড রক্তচাপ কমাতে সাহায্য করে এবং টমেটোর ত্বকে থাকা নারিনজেনিন প্রদাহ কমাতে সাহায্য করে।একটি সুস্থ ইমিউন সিস্টেম তৈরিতে সাহায্য করেটমেটো শরীরের ভিটামিন সি এর দৈনিক চাহিদার ২৫% পূরণ করতে পারে। প্রতি ১০০ গ্রামে ১৩.৭ মিলিগ্রাম ভিটামিন সি পাওয়া যায়। ভিটামিন সি একটি অ্যান্টিঅক্সিডেন্ট,যা ইমিউন কোষ তৈরি করতে সাহায্য করে এবং কোষ বৃদ্ধি এবং নিরাময় করে। ভিটামিন ই এর মতো শরীরের অন্যান্য অ্যান্টিঅক্সিডেন্টগুলির তৈরি করাও ভিটামিন সি'র উপর নির্ভর করে।নিরামিষাশী অর্থাৎ যারা প্রাণীজ খাবার থেকে আয়রন পান না তাদের জন্য ভিটামিন সি বিশেষভাবে গুরুত্বপূর্ণ। কারণ এটি শরীরকে উদ্ভিদ-ভিত্তিক খাবার থেকে আয়রন আরও সহজে শোষণ করতে সাহায্য করে।অন্ত্রের স্বাস্থ্যের উন্নতি করেটমেটো অদ্রবণীয় ফাইবারে পূর্ণ, যা মলকে নরম করে। যদি কোষ্ঠকাঠিন্যের সমস্যা থাকে তবে এটি বিশেষভাবে উপকারী। দ্রবণীয় ফাইবার থাকায় খাদ্যতালিকায় টমেটো অন্তর্ভুক্ত করলে এটি রক্তের গ্লুকোজ (চিনির) মাত্রা নিয়ন্ত্রণ করে। এটি কার্বোহাইড্রেটের হজম এবং শোষণকে ধীর করে দেয়, তাই রক্তে শর্করার মাত্রা খুব দ্রুত বাড়ে না এবং খাওয়ার পরে বেশি সময় ধরে পেট ভরা অনুভুতি দেয়। দ্রবণীয় ফাইবার আমাদের অন্ত্রে একটি প্রিবায়োটিক হিসাবেও কাজ করে।প্রিবায়োটিক হল এমন খাবার যা আমাদের স্বাস্থ্যকর অন্ত্রের ব্যাকটেরিয়াকে পুষ্ট করে। সুস্থ প্রাপ্তবয়স্ক নারীদের প্রতিদিন খাদ্যতালিকায় ২৫ গ্রাম এবং পুরুষদের ৩৮ গ্রাম ফাইবার রাখা উচিত। একটি মাঝারি টমেটো ১.৫ গ্রাম ফাইবার সরবরাহ করে, যা অদ্রবণীয় এবং দ্রবণীয় ফাইবার দ্বারা গঠিত। উভয়ই হজমের জন্য প্রয়োজনীয় এবং সহায়ক।আরওপাঙাশ মাছের পুষ্টিগুণ কি আসলেই কম?রক্তচাপ নিয়ন্ত্রণে সহায়কটমেটো আমাদের পটাসিয়ামের চাহিদা পূরণ করে স্বাস্থ্যকর রক্তচাপ নিয়ন্ত্রণেও সাহায্য করে। রক্তে পটাসিয়ামের উপস্থিতি কিডনির রক্তপ্রবাহে সোডিয়ামের মাত্রা কমিয়ে দেয়, যা রক্তচাপ কমিয়ে দেয়।রক্ত এবং হাড়ের স্বাস্থ্য উন্নয়নে সহায়কএকটি মাঝারি টমেটোতে ভিটামিন কে-এর দৈনিক চাহিদার প্রায় ১৫ শতাংশ থাকে। ভিটামিন কে প্রোটিন গঠন এবং সক্রিয় করতে ভূমিকা রাখে যা রক্ত \\u200b\\u200bজমাট বাঁধতে এবং হাড়কে শক্তিশালী করে। গবেষণায় দেখা গেছে যে রক্তে বেশি লাইকোপিন থাকলে হৃদরোগ এবং এমনকি কিছু ক্যানসারের ঝুঁকি হ্রাস করে।কোষের বৃদ্ধিতে ভূমিকা রাখেটমেটোতে রয়েছে ফোলেট বা ভিটামিন বি৯। এটি টিস্যুর স্বাভাবিক বৃদ্ধি এবং কোষের কার্যকারিতা সঠিকভাবে পরিচালনার জন্য বিশেষভাবে কাজ করে। যারা অন্তঃসত্ত্বা তাদের জন্য ফোলেট গর্ভাবস্থায় ক্রমবর্ধমান ভ্রূণের নিউরাল টিউব ত্রুটি প্রতিরোধে সহায়তা করে, যা অন্তঃসত্ত্বা নারীর জন্য গুরুত্বপূর্ণ।শ্বাসযন্ত্রের উন্নয়নটমেটোতে আছে লাইকোপিন, ক্যারোটিনয়েড যা ফুসফুস ভাল রাখার জন্য গুরুত্বপূর্ণ এবং এর সঠিক কার্যকারিতার সঙ্গে যুক্ত। প্রকৃতপক্ষে, এই অ্যান্টিঅক্সিডেন্ট ধূমপায়ীদের ফুসফুসের ক্যানসারের ঝুঁকি কমায়।ব্যায়ামের পর ক্লান্তি দূর করেটমেটোর রস পটাসিয়াম এবং সোডিয়ামের মতো ইলেক্ট্রোলাইটের একটি চমৎকার উৎস টমেটো। তীব্র, ঘামযুক্ত ওয়ার্কআউটের পরে টমেটোর রস পান করলে বা টমেটো খেলে শক্তি ফিরে পেতে সাহায্য করতে পারে এবং ক্লান্তির পরিবর্তে পুনরুজ্জীবিত করতে সাহায্য করতে পারে।স্ট্রোকের ঝুঁকি হ্রাস করেটমেটোতে থাকা লাইকোপিন উপাদান হার্টের কিছু জটিলতাও কমাতে পারে। যাদের রক্তে লাইকোপিনের পরিমাণ সবচেয়ে বেশি তাদের স্ট্রোকের ঝুঁকি কম দেখা গেছে। একটি সমীক্ষায় দেখা গেছে যে যাদের রক্তে সবচেয়ে কম পরিমাণে লাইকোপিন রয়েছে তাদের তুলনায় যাদের রক্তে বেশি পরিমাণে লাইকোপিন আছে তাদের স্ট্রোক হওয়ার ঝুঁকি ৫৫ শতাংশ কম।আরওডিমের কুসুম খাবেন নাকি খাবেন নাত্বক ভালো রাখেটমেটোতে থাকা অ্যান্টিঅক্সিডেন্টগুলি ত্বকে ইউভি রশ্মির ক্ষতিকারক প্রভাব প্রতিরোধ করতে সহায়তা করে। টমেটো ত্বকের বার্ধক্যের প্রভাব কমাতে সাহায্য করে। লাইকোপিন সূর্যের ক্ষতিকারক রশ্মি থেকে ত্বককে সুরক্ষা দেয়। এটি কোলাজেন ভেঙে যাওয়া প্রতিরোধে এবং আমাদের ত্বকে কোলাজেন উৎপাদন বাড়াতেও ভূমিকা রাখে।চোখ ভালো রাখেটমেটোতে থাকা আরেকটি অ্যান্টিঅক্সিডেন্ট হল বিটা-ক্যারোটিন, যা শরীরে ভিটামিন এ-তে রূপান্তরিত হয়। ভিটামিন এ চোখের স্বাস্থ্য এবং ইমিউন সিস্টেমের জন্য গুরুত্বপূর্ণ।সতর্কতাযদিও টমেটো সাধারণত একটি পুষ্টিকর এবং উপকারী খাবার হিসাবে বিবেচিত হয়, তবে কিছু শারীরিক অসুস্থতায় টমেটো সেবন কম করা উচিত।ইরিটেবল বাওয়েল সিনড্রোম (আইবিএস) আক্রান্ত ব্যক্তিদের প্রায়শই নির্দিষ্ট খাবারের প্রতি উচ্চ সংবেদনশীলতা থাকে। আইবিএস রোগীদের টমেটো খাওয়ার ফলে অস্বস্তিবোধ হতে পারে। আইবিএস আক্রান্ত ব্যক্তিদের তাদের উপসর্গগুলি পর্যবেক্ষণ করা হয় এবং প্রয়োজনে অস্বস্তি রোধ করার জন্য টমেটো খাওয়া সীমিত করার পরামর্শ দেওয়া হয়।টমেটোকে মাঝারি থেকে উচ্চ হিস্টামিন জাতীয় খাবার হিসাবে বিবেচনা করা হয়। হিস্টামিন অসহিষ্ণুতাযুক্ত ব্যক্তিরা টমেটো খাওয়ার পরে মাথাব্যথা, ত্বকে ফুসকুড়ি, নাক বন্ধ হওয়া বা গ্যাস্ট্রোইনটেস্টাইনাল ব্যাঘাতের মতো লক্ষণগুলি অনুভব করতে পারে।কিছু ধরনের কিডনিতে পাথর যেমন ক্যালসিয়াম অক্সালেট পাথর খাদ্যের কারণে তৈরি হতে পারে। টমেটোতে অক্সালেট থাকে যা কিডনিতে পাথর তৈরি করতে পারে।যদিও তুলনামূলকভাবে বিরল কিছু ব্যক্তির টমেটোতে অ্যালার্জি বা সংবেদনশীলতা হতে পারে। টমেটো অ্যালার্জির লক্ষণগুলো হালকা থেকে গুরুতর পর্যন্ত হতে পারে। এরমধ্যে ত্বকের ফুসকুড়ি, চুলকানি, আমবাত, ফোলাভাব বা গ্যাস্ট্রোইনটেস্টাইনাল ব্যাঘাত অন্তর্ভুক্ত থাকতে পারে।টমেটোতে অ্যাসিড বেশি থাকে, যা অ্যাসিড রিফ্লাক্সের সমস্যা বাড়িয়ে দিতে পারে।এ ছাড়া প্রচুর পরিমাণে টমেটো খাওয়ার ফলে অ্যালকালয়েড সোলানিনের কারণে হজমের সমস্যা এবং বমি বমি ভাব হতে পারে।আরওওজন কমানো থেকে ডায়াবেটিস নিয়ন্ত্রণ, ওটসের যত উপকার টমেটোতে ভিটামিন এবং খনিজ উপাদান রয়েছে যা স্বাস্থ্যকর। একটি বড় টমেটো (প্রায় ১৮২ গ্রাম) থেকে পাওয়া যায় - পানি- প্রায় ৯৫% শক্তি - ৩২.৮ কিলোক্যালরি প্রোটিন - ১.৬ গ্রাম চর্বি - ০.৩৬৪ গ্রাম কার্বোহাইড্রেট - ৭.০৮গ্রাম ফাইবার - ২.১৮ গ্রাম চিনি - ৪.৭৯ গ্রাম গ্লুকোজ - ২.২৮ গ্রাম ফ্রুক্টোজ - ২.৪৮ গ্রাম ক্যালসিয়াম - ১৮.২ মিলিগ্রাম আয়রন - ০.৪৯২ মিলিগ্রাম ম্যাগনেসিয়াম - ২০ মিলিগ্রাম ফসফরাস - ৪৩.৭ মিলিগ্রাম পটাসিয়াম - ৪৩১ মিলিগ্রাম সোডিয়াম - ৯.১ মিলিগ্রাম দস্তা - ০.৩০৯ মিলিগ্রাম তামা - ০.১০৭ মিলিগ্রাম ভিটামিন এ - ৪২ মাইক্রোগ্রাম ভিটামিন কে - ৭.৯ মাইক্রোগ্রাম টমেটোর উপকারিতা টমেটোর স্বাস্থ্য উপকারিতা অনেক। প্রতিদিনের খাদ্যতালিকায় টমেটো অন্তর্ভুক্ত করলে রক্তের ট্রাইগ্লিসারাইডের মাত্রা হ্রাস পায়। টমেটো পোস্টপ্র্যান্ডিয়াল লিপেমিয়া প্রতিরোধে সহায়তা করতে পারে এবং এটি করার ফলে এথেরোস্ক্লেরোসিস হওয়ার ঝুঁকি হ্রাস পায়। এ ছাড়া নিয়মিত টমেটো খেলে এটি ওজন কমাতে সাহায্য করে। অক্সিডেটিভ স্ট্রেস এবং প্রদাহ হ্রাসটমেটো লাইকোপিন, বিটা-ক্যারোটিন, লুটেইন, ক্লোরোজেনিক অ্যাসিড এবং নারিনজেনিন সহ বিভিন্ন অ্যান্টিঅক্সিডেন্টে পূর্ণ। অ্যান্টিঅক্সিডেন্টগুলি আমাদের শরীরের অক্সিডেটিভ স্ট্রেসের ফলে সৃষ্ট সমস্যা ঠিক করতে সাহায্য করে। দূষণ থেকে শুরু করে অতিবেগুনি রশ্মি অনেক কিছু এই অক্সিডেটিভ স্ট্রেসের কারণ হতে পারে। টমেটোতে লুটেইনও রয়েছে, যা বয়স-সম্পর্কিত ম্যাকুলার অবক্ষয় রোধ করতে সাহায্য করে। ক্লোরোজেনিক অ্যাসিড রক্তচাপ কমাতে সাহায্য করে এবং টমেটোর ত্বকে থাকা নারিনজেনিন প্রদাহ কমাতে সাহায্য করে।একটি সুস্থ ইমিউন সিস্টেম তৈরিতে সাহায্য করেটমেটো শরীরের ভিটামিন সি এর দৈনিক চাহিদার ২৫% পূরণ করতে পারে। প্রতি ১০০ গ্রামে ১৩.৭ মিলিগ্রাম ভিটামিন সি পাওয়া যায়। ভিটামিন সি একটি অ্যান্টিঅক্সিডেন্ট,যা ইমিউন কোষ তৈরি করতে সাহায্য করে এবং কোষ বৃদ্ধি এবং নিরাময় করে। ভিটামিন ই এর মতো শরীরের অন্যান্য অ্যান্টিঅক্সিডেন্টগুলির তৈরি করাও ভিটামিন সি'র উপর নির্ভর করে।নিরামিষাশী অর্থাৎ যারা প্রাণীজ খাবার থেকে আয়রন পান না তাদের জন্য ভিটামিন সি বিশেষভাবে গুরুত্বপূর্ণ। কারণ এটি শরীরকে উদ্ভিদ-ভিত্তিক খাবার থেকে আয়রন আরও সহজে শোষণ করতে সাহায্য করে।অন্ত্রের স্বাস্থ্যের উন্নতি করেটমেটো অদ্রবণীয় ফাইবারে পূর্ণ, যা মলকে নরম করে। যদি কোষ্ঠকাঠিন্যের সমস্যা থাকে তবে এটি বিশেষভাবে উপকারী। দ্রবণীয় ফাইবার থাকায় খাদ্যতালিকায় টমেটো অন্তর্ভুক্ত করলে এটি রক্তের গ্লুকোজ (চিনির) মাত্রা নিয়ন্ত্রণ করে। এটি কার্বোহাইড্রেটের হজম এবং শোষণকে ধীর করে দেয়, তাই রক্তে শর্করার মাত্রা খুব দ্রুত বাড়ে না এবং খাওয়ার পরে বেশি সময় ধরে পেট ভরা অনুভুতি দেয়। দ্রবণীয় ফাইবার আমাদের অন্ত্রে একটি প্রিবায়োটিক হিসাবেও কাজ করে।প্রিবায়োটিক হল এমন খাবার যা আমাদের স্বাস্থ্যকর অন্ত্রের ব্যাকটেরিয়াকে পুষ্ট করে। সুস্থ প্রাপ্তবয়স্ক নারীদের প্রতিদিন খাদ্যতালিকায় ২৫ গ্রাম এবং পুরুষদের ৩৮ গ্রাম ফাইবার রাখা উচিত। একটি মাঝারি টমেটো ১.৫ গ্রাম ফাইবার সরবরাহ করে, যা অদ্রবণীয় এবং দ্রবণীয় ফাইবার দ্বারা গঠিত। উভয়ই হজমের জন্য প্রয়োজনীয় এবং সহায়ক।আরওপাঙাশ মাছের পুষ্টিগুণ কি আসলেই কম?রক্তচাপ নিয়ন্ত্রণে সহায়কটমেটো আমাদের পটাসিয়ামের চাহিদা পূরণ করে স্বাস্থ্যকর রক্তচাপ নিয়ন্ত্রণেও সাহায্য করে। রক্তে পটাসিয়ামের উপস্থিতি কিডনির রক্তপ্রবাহে সোডিয়ামের মাত্রা কমিয়ে দেয়, যা রক্তচাপ কমিয়ে দেয়।রক্ত এবং হাড়ের স্বাস্থ্য উন্নয়নে সহায়কএকটি মাঝারি টমেটোতে ভিটামিন কে-এর দৈনিক চাহিদার প্রায় ১৫ শতাংশ থাকে। ভিটামিন কে প্রোটিন গঠন এবং সক্রিয় করতে ভূমিকা রাখে যা রক্ত \\u200b\\u200bজমাট বাঁধতে এবং হাড়কে শক্তিশালী করে। গবেষণায় দেখা গেছে যে রক্তে বেশি লাইকোপিন থাকলে হৃদরোগ এবং এমনকি কিছু ক্যানসারের ঝুঁকি হ্রাস করে।কোষের বৃদ্ধিতে ভূমিকা রাখেটমেটোতে রয়েছে ফোলেট বা ভিটামিন বি৯। এটি টিস্যুর স্বাভাবিক বৃদ্ধি এবং কোষের কার্যকারিতা সঠিকভাবে পরিচালনার জন্য বিশেষভাবে কাজ করে। যারা অন্তঃসত্ত্বা তাদের জন্য ফোলেট গর্ভাবস্থায় ক্রমবর্ধমান ভ্রূণের নিউরাল টিউব ত্রুটি প্রতিরোধে সহায়তা করে, যা অন্তঃসত্ত্বা নারীর জন্য গুরুত্বপূর্ণ।শ্বাসযন্ত্রের উন্নয়নটমেটোতে আছে লাইকোপিন, ক্যারোটিনয়েড যা ফুসফুস ভাল রাখার জন্য গুরুত্বপূর্ণ এবং এর সঠিক কার্যকারিতার সঙ্গে যুক্ত। প্রকৃতপক্ষে, এই অ্যান্টিঅক্সিডেন্ট ধূমপায়ীদের ফুসফুসের ক্যানসারের ঝুঁকি কমায়।ব্যায়ামের পর ক্লান্তি দূর করেটমেটোর রস পটাসিয়াম এবং সোডিয়ামের মতো ইলেক্ট্রোলাইটের একটি চমৎকার উৎস টমেটো। তীব্র, ঘামযুক্ত ওয়ার্কআউটের পরে টমেটোর রস পান করলে বা টমেটো খেলে শক্তি ফিরে পেতে সাহায্য করতে পারে এবং ক্লান্তির পরিবর্তে পুনরুজ্জীবিত করতে সাহায্য করতে পারে।স্ট্রোকের ঝুঁকি হ্রাস করেটমেটোতে থাকা লাইকোপিন উপাদান হার্টের কিছু জটিলতাও কমাতে পারে। যাদের রক্তে লাইকোপিনের পরিমাণ সবচেয়ে বেশি তাদের স্ট্রোকের ঝুঁকি কম দেখা গেছে। একটি সমীক্ষায় দেখা গেছে যে যাদের রক্তে সবচেয়ে কম পরিমাণে লাইকোপিন রয়েছে তাদের তুলনায় যাদের রক্তে বেশি পরিমাণে লাইকোপিন আছে তাদের স্ট্রোক হওয়ার ঝুঁকি ৫৫ শতাংশ কম।আরওডিমের কুসুম খাবেন নাকি খাবেন নাত্বক ভালো রাখেটমেটোতে থাকা অ্যান্টিঅক্সিডেন্টগুলি ত্বকে ইউভি রশ্মির ক্ষতিকারক প্রভাব প্রতিরোধ করতে সহায়তা করে। টমেটো ত্বকের বার্ধক্যের প্রভাব কমাতে সাহায্য করে। লাইকোপিন সূর্যের ক্ষতিকারক রশ্মি থেকে ত্বককে সুরক্ষা দেয়। এটি কোলাজেন ভেঙে যাওয়া প্রতিরোধে এবং আমাদের ত্বকে কোলাজেন উৎপাদন বাড়াতেও ভূমিকা রাখে।চোখ ভালো রাখেটমেটোতে থাকা আরেকটি অ্যান্টিঅক্সিডেন্ট হল বিটা-ক্যারোটিন, যা শরীরে ভিটামিন এ-তে রূপান্তরিত হয়। ভিটামিন এ চোখের স্বাস্থ্য এবং ইমিউন সিস্টেমের জন্য গুরুত্বপূর্ণ।সতর্কতাযদিও টমেটো সাধারণত একটি পুষ্টিকর এবং উপকারী খাবার হিসাবে বিবেচিত হয়, তবে কিছু শারীরিক অসুস্থতায় টমেটো সেবন কম করা উচিত।ইরিটেবল বাওয়েল সিনড্রোম (আইবিএস) আক্রান্ত ব্যক্তিদের প্রায়শই নির্দিষ্ট খাবারের প্রতি উচ্চ সংবেদনশীলতা থাকে। আইবিএস রোগীদের টমেটো খাওয়ার ফলে অস্বস্তিবোধ হতে পারে। আইবিএস আক্রান্ত ব্যক্তিদের তাদের উপসর্গগুলি পর্যবেক্ষণ করা হয় এবং প্রয়োজনে অস্বস্তি রোধ করার জন্য টমেটো খাওয়া সীমিত করার পরামর্শ দেওয়া হয়।টমেটোকে মাঝারি থেকে উচ্চ হিস্টামিন জাতীয় খাবার হিসাবে বিবেচনা করা হয়। হিস্টামিন অসহিষ্ণুতাযুক্ত ব্যক্তিরা টমেটো খাওয়ার পরে মাথাব্যথা, ত্বকে ফুসকুড়ি, নাক বন্ধ হওয়া বা গ্যাস্ট্রোইনটেস্টাইনাল ব্যাঘাতের মতো লক্ষণগুলি অনুভব করতে পারে।কিছু ধরনের কিডনিতে পাথর যেমন ক্যালসিয়াম অক্সালেট পাথর খাদ্যের কারণে তৈরি হতে পারে। টমেটোতে অক্সালেট থাকে যা কিডনিতে পাথর তৈরি করতে পারে।যদিও তুলনামূলকভাবে বিরল কিছু ব্যক্তির টমেটোতে অ্যালার্জি বা সংবেদনশীলতা হতে পারে। টমেটো অ্যালার্জির লক্ষণগুলো হালকা থেকে গুরুতর পর্যন্ত হতে পারে। এরমধ্যে ত্বকের ফুসকুড়ি, চুলকানি, আমবাত, ফোলাভাব বা গ্যাস্ট্রোইনটেস্টাইনাল ব্যাঘাত অন্তর্ভুক্ত থাকতে পারে।টমেটোতে অ্যাসিড বেশি থাকে, যা অ্যাসিড রিফ্লাক্সের সমস্যা বাড়িয়ে দিতে পারে।এ ছাড়া প্রচুর পরিমাণে টমেটো খাওয়ার ফলে অ্যালকালয়েড সোলানিনের কারণে হজমের সমস্যা এবং বমি বমি ভাব হতে পারে।আরওওজন কমানো থেকে ডায়াবেটিস নিয়ন্ত্রণ, ওটসের যত উপকার টমেটো লাইকোপিন, বিটা-ক্যারোটিন, লুটেইন, ক্লোরোজেনিক অ্যাসিড এবং নারিনজেনিন সহ বিভিন্ন অ্যান্টিঅক্সিডেন্টে পূর্ণ। অ্যান্টিঅক্সিডেন্টগুলি আমাদের শরীরের অক্সিডেটিভ স্ট্রেসের ফলে সৃষ্ট সমস্যা ঠিক করতে সাহায্য করে। দূষণ থেকে শুরু করে অতিবেগুনি রশ্মি অনেক কিছু এই অক্সিডেটিভ স্ট্রেসের কারণ হতে পারে। টমেটোতে লুটেইনও রয়েছে, যা বয়স-সম্পর্কিত ম্যাকুলার অবক্ষয় রোধ করতে সাহায্য করে। ক্লোরোজেনিক অ্যাসিড রক্তচাপ কমাতে সাহায্য করে এবং টমেটোর ত্বকে থাকা নারিনজেনিন প্রদাহ কমাতে সাহায্য করে। একটি সুস্থ ইমিউন সিস্টেম তৈরিতে সাহায্য করে টমেটো শরীরের ভিটামিন সি এর দৈনিক চাহিদার ২৫% পূরণ করতে পারে। প্রতি ১০০ গ্রামে ১৩.৭ মিলিগ্রাম ভিটামিন সি পাওয়া যায়। ভিটামিন সি একটি অ্যান্টিঅক্সিডেন্ট,যা ইমিউন কোষ তৈরি করতে সাহায্য করে এবং কোষ বৃদ্ধি এবং নিরাময় করে। ভিটামিন ই এর মতো শরীরের অন্যান্য অ্যান্টিঅক্সিডেন্টগুলির তৈরি করাও ভিটামিন সি'র উপর নির্ভর করে। নিরামিষাশী অর্থাৎ যারা প্রাণীজ খাবার থেকে আয়রন পান না তাদের জন্য ভিটামিন সি বিশেষভাবে গুরুত্বপূর্ণ। কারণ এটি শরীরকে উদ্ভিদ-ভিত্তিক খাবার থেকে আয়রন আরও সহজে শোষণ করতে সাহায্য করে। অন্ত্রের স্বাস্থ্যের উন্নতি করে টমেটো অদ্রবণীয় ফাইবারে পূর্ণ, যা মলকে নরম করে। যদি কোষ্ঠকাঠিন্যের সমস্যা থাকে তবে এটি বিশেষভাবে উপকারী। দ্রবণীয় ফাইবার থাকায় খাদ্যতালিকায় টমেটো অন্তর্ভুক্ত করলে এটি রক্তের গ্লুকোজ (চিনির) মাত্রা নিয়ন্ত্রণ করে। এটি কার্বোহাইড্রেটের হজম এবং শোষণকে ধীর করে দেয়, তাই রক্তে শর্করার মাত্রা খুব দ্রুত বাড়ে না এবং খাওয়ার পরে বেশি সময় ধরে পেট ভরা অনুভুতি দেয়। দ্রবণীয় ফাইবার আমাদের অন্ত্রে একটি প্রিবায়োটিক হিসাবেও কাজ করে।প্রিবায়োটিক হল এমন খাবার যা আমাদের স্বাস্থ্যকর অন্ত্রের ব্যাকটেরিয়াকে পুষ্ট করে। সুস্থ প্রাপ্তবয়স্ক নারীদের প্রতিদিন খাদ্যতালিকায় ২৫ গ্রাম এবং পুরুষদের ৩৮ গ্রাম ফাইবার রাখা উচিত। একটি মাঝারি টমেটো ১.৫ গ্রাম ফাইবার সরবরাহ করে, যা অদ্রবণীয় এবং দ্রবণীয় ফাইবার দ্বারা গঠিত। উভয়ই হজমের জন্য প্রয়োজনীয় এবং সহায়ক। রক্তচাপ নিয়ন্ত্রণে সহায়কটমেটো আমাদের পটাসিয়ামের চাহিদা পূরণ করে স্বাস্থ্যকর রক্তচাপ নিয়ন্ত্রণেও সাহায্য করে। রক্তে পটাসিয়ামের উপস্থিতি কিডনির রক্তপ্রবাহে সোডিয়ামের মাত্রা কমিয়ে দেয়, যা রক্তচাপ কমিয়ে দেয়।রক্ত এবং হাড়ের স্বাস্থ্য উন্নয়নে সহায়কএকটি মাঝারি টমেটোতে ভিটামিন কে-এর দৈনিক চাহিদার প্রায় ১৫ শতাংশ থাকে। ভিটামিন কে প্রোটিন গঠন এবং সক্রিয় করতে ভূমিকা রাখে যা রক্ত \\u200b\\u200bজমাট বাঁধতে এবং হাড়কে শক্তিশালী করে। গবেষণায় দেখা গেছে যে রক্তে বেশি লাইকোপিন থাকলে হৃদরোগ এবং এমনকি কিছু ক্যানসারের ঝুঁকি হ্রাস করে।কোষের বৃদ্ধিতে ভূমিকা রাখেটমেটোতে রয়েছে ফোলেট বা ভিটামিন বি৯। এটি টিস্যুর স্বাভাবিক বৃদ্ধি এবং কোষের কার্যকারিতা সঠিকভাবে পরিচালনার জন্য বিশেষভাবে কাজ করে। যারা অন্তঃসত্ত্বা তাদের জন্য ফোলেট গর্ভাবস্থায় ক্রমবর্ধমান ভ্রূণের নিউরাল টিউব ত্রুটি প্রতিরোধে সহায়তা করে, যা অন্তঃসত্ত্বা নারীর জন্য গুরুত্বপূর্ণ।শ্বাসযন্ত্রের উন্নয়নটমেটোতে আছে লাইকোপিন, ক্যারোটিনয়েড যা ফুসফুস ভাল রাখার জন্য গুরুত্বপূর্ণ এবং এর সঠিক কার্যকারিতার সঙ্গে যুক্ত। প্রকৃতপক্ষে, এই অ্যান্টিঅক্সিডেন্ট ধূমপায়ীদের ফুসফুসের ক্যানসারের ঝুঁকি কমায়।ব্যায়ামের পর ক্লান্তি দূর করেটমেটোর রস পটাসিয়াম এবং সোডিয়ামের মতো ইলেক্ট্রোলাইটের একটি চমৎকার উৎস টমেটো। তীব্র, ঘামযুক্ত ওয়ার্কআউটের পরে টমেটোর রস পান করলে বা টমেটো খেলে শক্তি ফিরে পেতে সাহায্য করতে পারে এবং ক্লান্তির পরিবর্তে পুনরুজ্জীবিত করতে সাহায্য করতে পারে।স্ট্রোকের ঝুঁকি হ্রাস করেটমেটোতে থাকা লাইকোপিন উপাদান হার্টের কিছু জটিলতাও কমাতে পারে। যাদের রক্তে লাইকোপিনের পরিমাণ সবচেয়ে বেশি তাদের স্ট্রোকের ঝুঁকি কম দেখা গেছে। একটি সমীক্ষায় দেখা গেছে যে যাদের রক্তে সবচেয়ে কম পরিমাণে লাইকোপিন রয়েছে তাদের তুলনায় যাদের রক্তে বেশি পরিমাণে লাইকোপিন আছে তাদের স্ট্রোক হওয়ার ঝুঁকি ৫৫ শতাংশ কম।আরওডিমের কুসুম খাবেন নাকি খাবেন নাত্বক ভালো রাখেটমেটোতে থাকা অ্যান্টিঅক্সিডেন্টগুলি ত্বকে ইউভি রশ্মির ক্ষতিকারক প্রভাব প্রতিরোধ করতে সহায়তা করে। টমেটো ত্বকের বার্ধক্যের প্রভাব কমাতে সাহায্য করে। লাইকোপিন সূর্যের ক্ষতিকারক রশ্মি থেকে ত্বককে সুরক্ষা দেয়। এটি কোলাজেন ভেঙে যাওয়া প্রতিরোধে এবং আমাদের ত্বকে কোলাজেন উৎপাদন বাড়াতেও ভূমিকা রাখে।চোখ ভালো রাখেটমেটোতে থাকা আরেকটি অ্যান্টিঅক্সিডেন্ট হল বিটা-ক্যারোটিন, যা শরীরে ভিটামিন এ-তে রূপান্তরিত হয়। ভিটামিন এ চোখের স্বাস্থ্য এবং ইমিউন সিস্টেমের জন্য গুরুত্বপূর্ণ।সতর্কতাযদিও টমেটো সাধারণত একটি পুষ্টিকর এবং উপকারী খাবার হিসাবে বিবেচিত হয়, তবে কিছু শারীরিক অসুস্থতায় টমেটো সেবন কম করা উচিত।ইরিটেবল বাওয়েল সিনড্রোম (আইবিএস) আক্রান্ত ব্যক্তিদের প্রায়শই নির্দিষ্ট খাবারের প্রতি উচ্চ সংবেদনশীলতা থাকে। আইবিএস রোগীদের টমেটো খাওয়ার ফলে অস্বস্তিবোধ হতে পারে। আইবিএস আক্রান্ত ব্যক্তিদের তাদের উপসর্গগুলি পর্যবেক্ষণ করা হয় এবং প্রয়োজনে অস্বস্তি রোধ করার জন্য টমেটো খাওয়া সীমিত করার পরামর্শ দেওয়া হয়।টমেটোকে মাঝারি থেকে উচ্চ হিস্টামিন জাতীয় খাবার হিসাবে বিবেচনা করা হয়। হিস্টামিন অসহিষ্ণুতাযুক্ত ব্যক্তিরা টমেটো খাওয়ার পরে মাথাব্যথা, ত্বকে ফুসকুড়ি, নাক বন্ধ হওয়া বা গ্যাস্ট্রোইনটেস্টাইনাল ব্যাঘাতের মতো লক্ষণগুলি অনুভব করতে পারে।কিছু ধরনের কিডনিতে পাথর যেমন ক্যালসিয়াম অক্সালেট পাথর খাদ্যের কারণে তৈরি হতে পারে। টমেটোতে অক্সালেট থাকে যা কিডনিতে পাথর তৈরি করতে পারে।যদিও তুলনামূলকভাবে বিরল কিছু ব্যক্তির টমেটোতে অ্যালার্জি বা সংবেদনশীলতা হতে পারে। টমেটো অ্যালার্জির লক্ষণগুলো হালকা থেকে গুরুতর পর্যন্ত হতে পারে। এরমধ্যে ত্বকের ফুসকুড়ি, চুলকানি, আমবাত, ফোলাভাব বা গ্যাস্ট্রোইনটেস্টাইনাল ব্যাঘাত অন্তর্ভুক্ত থাকতে পারে।টমেটোতে অ্যাসিড বেশি থাকে, যা অ্যাসিড রিফ্লাক্সের সমস্যা বাড়িয়ে দিতে পারে।এ ছাড়া প্রচুর পরিমাণে টমেটো খাওয়ার ফলে অ্যালকালয়েড সোলানিনের কারণে হজমের সমস্যা এবং বমি বমি ভাব হতে পারে।আরওওজন কমানো থেকে ডায়াবেটিস নিয়ন্ত্রণ, ওটসের যত উপকার টমেটো আমাদের পটাসিয়ামের চাহিদা পূরণ করে স্বাস্থ্যকর রক্তচাপ নিয়ন্ত্রণেও সাহায্য করে। রক্তে পটাসিয়ামের উপস্থিতি কিডনির রক্তপ্রবাহে সোডিয়ামের মাত্রা কমিয়ে দেয়, যা রক্তচাপ কমিয়ে দেয়। রক্ত এবং হাড়ের স্বাস্থ্য উন্নয়নে সহায়ক একটি মাঝারি টমেটোতে ভিটামিন কে-এর দৈনিক চাহিদার প্রায় ১৫ শতাংশ থাকে। ভিটামিন কে প্রোটিন গঠন এবং সক্রিয় করতে ভূমিকা রাখে যা রক্ত \\u200b\\u200bজমাট বাঁধতে এবং হাড়কে শক্তিশালী করে। গবেষণায় দেখা গেছে যে রক্তে বেশি লাইকোপিন থাকলে হৃদরোগ এবং এমনকি কিছু ক্যানসারের ঝুঁকি হ্রাস করে। কোষের বৃদ্ধিতে ভূমিকা রাখে টমেটোতে রয়েছে ফোলেট বা ভিটামিন বি৯। এটি টিস্যুর স্বাভাবিক বৃদ্ধি এবং কোষের কার্যকারিতা সঠিকভাবে পরিচালনার জন্য বিশেষভাবে কাজ করে। যারা অন্তঃসত্ত্বা তাদের জন্য ফোলেট গর্ভাবস্থায় ক্রমবর্ধমান ভ্রূণের নিউরাল টিউব ত্রুটি প্রতিরোধে সহায়তা করে, যা অন্তঃসত্ত্বা নারীর জন্য গুরুত্বপূর্ণ। শ্বাসযন্ত্রের উন্নয়ন টমেটোতে আছে লাইকোপিন, ক্যারোটিনয়েড যা ফুসফুস ভাল রাখার জন্য গুরুত্বপূর্ণ এবং এর সঠিক কার্যকারিতার সঙ্গে যুক্ত। প্রকৃতপক্ষে, এই অ্যান্টিঅক্সিডেন্ট ধূমপায়ীদের ফুসফুসের ক্যানসারের ঝুঁকি কমায়। ব্যায়ামের পর ক্লান্তি দূর করে টমেটোর রস পটাসিয়াম এবং সোডিয়ামের মতো ইলেক্ট্রোলাইটের একটি চমৎকার উৎস টমেটো। তীব্র, ঘামযুক্ত ওয়ার্কআউটের পরে টমেটোর রস পান করলে বা টমেটো খেলে শক্তি ফিরে পেতে সাহায্য করতে পারে এবং ক্লান্তির পরিবর্তে পুনরুজ্জীবিত করতে সাহায্য করতে পারে। স্ট্রোকের ঝুঁকি হ্রাস করে টমেটোতে থাকা লাইকোপিন উপাদান হার্টের কিছু জটিলতাও কমাতে পারে। যাদের রক্তে লাইকোপিনের পরিমাণ সবচেয়ে বেশি তাদের স্ট্রোকের ঝুঁকি কম দেখা গেছে। একটি সমীক্ষায় দেখা গেছে যে যাদের রক্তে সবচেয়ে কম পরিমাণে লাইকোপিন রয়েছে তাদের তুলনায় যাদের রক্তে বেশি পরিমাণে লাইকোপিন আছে তাদের স্ট্রোক হওয়ার ঝুঁকি ৫৫ শতাংশ কম। ত্বক ভালো রাখেটমেটোতে থাকা অ্যান্টিঅক্সিডেন্টগুলি ত্বকে ইউভি রশ্মির ক্ষতিকারক প্রভাব প্রতিরোধ করতে সহায়তা করে। টমেটো ত্বকের বার্ধক্যের প্রভাব কমাতে সাহায্য করে। লাইকোপিন সূর্যের ক্ষতিকারক রশ্মি থেকে ত্বককে সুরক্ষা দেয়। এটি কোলাজেন ভেঙে যাওয়া প্রতিরোধে এবং আমাদের ত্বকে কোলাজেন উৎপাদন বাড়াতেও ভূমিকা রাখে।চোখ ভালো রাখেটমেটোতে থাকা আরেকটি অ্যান্টিঅক্সিডেন্ট হল বিটা-ক্যারোটিন, যা শরীরে ভিটামিন এ-তে রূপান্তরিত হয়। ভিটামিন এ চোখের স্বাস্থ্য এবং ইমিউন সিস্টেমের জন্য গুরুত্বপূর্ণ।সতর্কতাযদিও টমেটো সাধারণত একটি পুষ্টিকর এবং উপকারী খাবার হিসাবে বিবেচিত হয়, তবে কিছু শারীরিক অসুস্থতায় টমেটো সেবন কম করা উচিত।ইরিটেবল বাওয়েল সিনড্রোম (আইবিএস) আক্রান্ত ব্যক্তিদের প্রায়শই নির্দিষ্ট খাবারের প্রতি উচ্চ সংবেদনশীলতা থাকে। আইবিএস রোগীদের টমেটো খাওয়ার ফলে অস্বস্তিবোধ হতে পারে। আইবিএস আক্রান্ত ব্যক্তিদের তাদের উপসর্গগুলি পর্যবেক্ষণ করা হয় এবং প্রয়োজনে অস্বস্তি রোধ করার জন্য টমেটো খাওয়া সীমিত করার পরামর্শ দেওয়া হয়।টমেটোকে মাঝারি থেকে উচ্চ হিস্টামিন জাতীয় খাবার হিসাবে বিবেচনা করা হয়। হিস্টামিন অসহিষ্ণুতাযুক্ত ব্যক্তিরা টমেটো খাওয়ার পরে মাথাব্যথা, ত্বকে ফুসকুড়ি, নাক বন্ধ হওয়া বা গ্যাস্ট্রোইনটেস্টাইনাল ব্যাঘাতের মতো লক্ষণগুলি অনুভব করতে পারে।কিছু ধরনের কিডনিতে পাথর যেমন ক্যালসিয়াম অক্সালেট পাথর খাদ্যের কারণে তৈরি হতে পারে। টমেটোতে অক্সালেট থাকে যা কিডনিতে পাথর তৈরি করতে পারে।যদিও তুলনামূলকভাবে বিরল কিছু ব্যক্তির টমেটোতে অ্যালার্জি বা সংবেদনশীলতা হতে পারে। টমেটো অ্যালার্জির লক্ষণগুলো হালকা থেকে গুরুতর পর্যন্ত হতে পারে। এরমধ্যে ত্বকের ফুসকুড়ি, চুলকানি, আমবাত, ফোলাভাব বা গ্যাস্ট্রোইনটেস্টাইনাল ব্যাঘাত অন্তর্ভুক্ত থাকতে পারে।টমেটোতে অ্যাসিড বেশি থাকে, যা অ্যাসিড রিফ্লাক্সের সমস্যা বাড়িয়ে দিতে পারে।এ ছাড়া প্রচুর পরিমাণে টমেটো খাওয়ার ফলে অ্যালকালয়েড সোলানিনের কারণে হজমের সমস্যা এবং বমি বমি ভাব হতে পারে।আরওওজন কমানো থেকে ডায়াবেটিস নিয়ন্ত্রণ, ওটসের যত উপকার টমেটোতে থাকা অ্যান্টিঅক্সিডেন্টগুলি ত্বকে ইউভি রশ্মির ক্ষতিকারক প্রভাব প্রতিরোধ করতে সহায়তা করে। টমেটো ত্বকের বার্ধক্যের প্রভাব কমাতে সাহায্য করে। লাইকোপিন সূর্যের ক্ষতিকারক রশ্মি থেকে ত্বককে সুরক্ষা দেয়। এটি কোলাজেন ভেঙে যাওয়া প্রতিরোধে এবং আমাদের ত্বকে কোলাজেন উৎপাদন বাড়াতেও ভূমিকা রাখে। চোখ ভালো রাখে টমেটোতে থাকা আরেকটি অ্যান্টিঅক্সিডেন্ট হল বিটা-ক্যারোটিন, যা শরীরে ভিটামিন এ-তে রূপান্তরিত হয়। ভিটামিন এ চোখের স্বাস্থ্য এবং ইমিউন সিস্টেমের জন্য গুরুত্বপূর্ণ। সতর্কতা\"}\n",
            "- {'title': '‘বারবার তাদের আমন্ত্রণ জানাচ্ছি, কিন্তু তারা ওখান থেকেই কল্পকাহিনী বানিয়ে যাচ্ছে’', 'news_link': 'https://bangla.thedailystar.net/news/bangladesh/news-634116', 'news_text': \"অন্তর্বর্তী সরকারের প্রধান উপদেষ্টা অধ্যাপক ড. মুহাম্মদ ইউনূস বলেছেন, দেশের বর্তমান বাস্তবতা বিশ্বের কাছে প্রতিষ্ঠিত করতে সবাইকে একজোট থাকতে হবে। আজ বুধবার রাজধানীর ফরেন সার্ভিস একাডেমিতে রাজনৈতিক দলের নেতাদের সঙ্গে বৈঠকে এ কথা বলেন তিনি। অধ্যাপক ইউনূস বলেন, 'আমাদের এই স্বাধীনতা অনেকের কাছে পছন্দ হচ্ছে না। নানাভাবে এটাকে উল্টে দেওয়ার চেষ্টা চলছে। ৫ আগস্টের পর থেকে নানাভাবে এটা চলছে। সারাদেশে শান্তিপূর্ণভাবে দুর্গাপূজা পালন হয়েছে। আমরাও পূজার আনন্দে শরিক হয়েছিলাম। কোথাও কোনো ধরনের বিশৃঙ্খলা হয়নি। সেটাও অনেকের পছন্দ হয়নি। দেশকে নতুন করে অস্থির করার চেষ্টা চলছে।' তিনি বলেন, 'যে বাংলাদেশ আমরা গড়ে তোলার চেষ্টা করছি, সেটাকে ধামাচাপা দিয়ে আরেক বাংলাদেশের কাহিনী রচনা করে যাচ্ছে। সারাক্ষণ নানা রূপে তারা এটা করে যাচ্ছে। এটা যে এখন এক দেশের মধ্যে আছে তা নয়, বিশেষ বিশেষ বড় দেশের মধ্যে ছড়িয়ে গেছে।' 'আমাদের এই অভ্যুত্থান যাদের পছন্দ হয়নি, তারা এটাকে মুছে দিতে চায়, এটাকে নতুন ভঙ্গীতে দুনিয়ার সামনে পেশ করতে চায়। আমাদের এখানে নাকি ভয়ঙ্কর কাণ্ড ঘটছে, তা থেকে বাংলাদেশকে রক্ষা করতে হবে। রক্ষার জন্য তারা এগিয়ে আসতে চায়। এখন সেগুলোকে মিথ্যা প্রমাণ করা বা বাস্তবতাকে প্রতিষ্ঠিত করার জন্য আমাদের সবাইকে একজোট হতে হবে। এটা কোনো বিশেষ রাজনৈতিক মতবাদের বিষয় না। জাতি হিসেবে আমাদের অস্তিত্বের বিষয়,' যোগ করেন তিনি। বাংলাদেশ নিয়ে সম্প্রতি ভারতের বিভিন্ন প্রোপাগান্ডার ইঙ্গিত করে প্রধান উপদেষ্টা বলেন, 'আমরা যে মুক্ত স্বাধীন বাংলাদেশ তৈরি করলাম, তারা এটাকে মুছে দিয়ে আগেরটায় ফিরে যেতে চায়। মুখে বলছে না যে আগেরটা, কিন্তু ভঙ্গী হলো আগেরটা ভালো ছিল। তাদের শক্তি এত বেশি যে তারা মানুষকে এর ভেতরে ভেড়াতে পারছে। তাদের কল্পকাহিনীর কারণে মানুষ সন্দেহ প্রকাশ করছে যে এটা কী ধরনের সরকার হলো।' 'আমরা বারেবারে তাদের বলছি যে আপনারা আসেন এখানে, দেখেন, এখানে কোনো বাঁধা নেই। কিন্তু না, তারা ওখান থেকেই কল্পকাহিনী বানিয়ে যাচ্ছে। এখন আমাদের সাড়া দুনিয়াকে বলতে হবে যে, আমরা এক, আমরা যেটা পেয়েছি সেটা একজোট হয়ে পেয়েছি, কোনো মতবাদের কারণে পাইনি, ধাক্কাধাক্কি করে পাইনি, যারা আমাদের ওপর চেপে ছিল, তাদের উপড়ে ফেলেছি। এটাই সবার সামনে তুলে ধরতে হবে, সবাই মিলে যেন এটা করতে পারি। আমাদের নতুন বাংলাদেশের যাত্রাপথে এটা মস্ত বড় একটা বিষয় হয়ে দাঁড়িয়েছে, অস্তিত্বের বিষয় হয়ে দাঁড়িয়েছে,' বলেন তিনি। রাজনৈতিক দলের নেতাদের উদ্দেশে ড. ইউনূস বলেন, 'আপনারা সবাই ভালো বোঝেন। সবাই মিলে আমরা একজোট হয়ে যেন কাজটা করতে পারি, সবাই একত্র হয়ে বললে একটা সমবেত শক্তি তৈরি হয়, এই সমবেত শক্তির জন্যই আপনাদের সঙ্গে বসা। বিকেল ৪টায় বিএনপি, এছাড়া, জামায়াতে ইসলামী, এবি পার্টি, গণঅধিকার পরিষদসহ আরও রাজনৈতিক দলের নেতাদের সঙ্গে ফরেন সার্ভিস একাডেমিতে এ বৈঠক করেন প্রধান উপদেষ্টা।\"}\n",
            "- {'title': 'দলের কথা ভেবেই পেনাল্টি ছেড়েছেন এমবাপে, দাবি কোচের ', 'news_link': 'https://bangla.thedailystar.net/sports/football/news-634086', 'news_text': \"কিলিয়ান এমবাপে কী পেনাল্টি নিতে ভয় পাচ্ছিলেন? নাকি নিজের উপর আত্মবিশ্বাস নেই। সংবাদ সম্মেলনে এমনভাবেই করা হলো প্রশ্নটা। তবে কার্লো আনচেলত্তি পাশে দাঁড়ালেন তার শিষ্যের পক্ষেই। দলের কথা ভেবে এমবাপে এমন সিদ্ধান্ত নিয়েছেন বলে দাবি করেন এই ইতালিয়ান কোচ। চোটের কারণে ভিনিসিয়ুস জুনিয়র না থাকায় রিয়াল মাদ্রিদের প্রথম পেনাল্টি টেকার এখন এমবাপেই। কিন্তু তারপরও গত শনিবার সান্তিয়াগো বার্নাব্যুতে লা লিগার ম্যাচে গেতাফের বিপক্ষে পেনাল্টি পেলে তা নিজে না নিয়ে জুড বেলিংহ্যামকে দেন এই ফরাসি। ম্যাচ তখন গোলশূন্য সমতায় ছিল। তাতেই আলোচনা হচ্ছে বিষয়টি নিয়ে। কারণ এর আগে চ্যাম্পিয়ন্স লিগে নিজেদের সবশেষ ম্যাচে লিভারপুলের বিপক্ষে ১-০ ব্যবধানে পিছিয়ে থাকার পর পেনাল্টি পেয়েছিল রিয়াল। যা থেকে গোল আদায় করতে পারেননি এমবাপে। শেষ পর্যন্ত সেদিন হেরেই যায় রিয়াল। তখন গোলটি করতে পারলে হয়তো ভিন্ন ফলও হতে পারতো। ম্যাচশেষে ড্রেসিংরুমে তাকে বিধ্বস্ত অবস্থাতেই দেখা গিয়েছিল। অ্যাথলেতিক বিলবাওর বিপক্ষে নামার আগে সংবাদ সম্মেলনে উঠে আসে সেই প্রসঙ্গ। তবে শিষ্যের পক্ষে ঢাল ধরে কোচ আনচেলত্তি বলেন, 'এখানে বিতর্কের কিছু নেই। এর দুটি দিক রয়েছে। গেতাফের বিপক্ষে এমবাপে যা করেছে, আপনি এটিকে তার জড়তা হিসাবে দেখতে পারেন, অথবা আপনি এটিকে আমাদের মতো দেখতে পারেন, যা হল দায়িত্ব।' 'এটা দলগত স্বার্থ এগিয়ে রাখার মত একটি কাজ। সে অসাধারণ প্রতিভাসম্পন্ন একজন খেলোয়াড়। সে হয়ত ফুটবলের সবচেয়ে বড় প্রতিভা হতে পারে, তবে ব্যক্তিগতভাবে দলকে বেশি গুরুত্ব দিয়েছে, আমি এটাকে অনেক মূল্যবান বলে মনে করি,' যোগ করেন এই ইতালিয়ান কোচ। নিজের সামর্থ্যের প্রতি বিশ্বাসের ঘাটতি কি-না জানতে চাইলে আবারও এই কোচ বলেন, 'আমি ব্যাপারটা সেভাবে দেখি না। ড্রেসিংরুমে এটা দলের জন্যই, তার সতীর্থদের জন্য, আমাদের সবার জন্যই বেশ গুরুত্বপূর্ণ। যেমনটা আগেই বলেছি, আমরা তার এই কাজের মূল্য দিই। দলের প্রতি নিবেদনের এটা একটা দুর্দান্ত নজির।' চলতি মৌসুমের শুরুতে রিয়াল মাদ্রিদে যোগ দেওয়ার পর এখনও সেভাবে নিজেকে মেলে ধরতে পারছেন না এমবাপে। যদিও নিজের পছন্দের পজিশনে খেলতে পারছেন না। এখন পর্যন্ত ১০টি গোল এসেছে তার কাছ থেকে।\"}\n",
            "- {'title': 'বাংলাদেশের সার্বভৌমত্বের প্রতি ভারতকে শ্রদ্ধাশীল হওয়ার আহ্বান মির্জা ফখরুলের', 'news_link': 'https://bangla.thedailystar.net/news/bangladesh/politics/news-634021', 'news_text': \"বাংলাদেশের সার্বভৌমত্ব ও গণতন্ত্রের প্রতি ভারতকে শ্রদ্ধাশীল হওয়ার আহ্বান জানিয়েছেন বিএনপি মহাসচিব মির্জা ফখরুল ইসলাম আলমগীর। তিনি বলেছেন, যারা অনেক ত্যাগের মাধ্যমে স্বাধীনতা ও গণতন্ত্র অর্জন করেছে তাদের বিরুদ্ধে অবস্থান নেবেন না। আজ বাংলাদেশ সময় রাত ২টায় লন্ডনে এক সমাবেশে তিনি এ কথা বলেন। তিনি বলেন, ১৯৭১ সালে যুদ্ধের সময় আমাদের সহযোগিতা করেছিল। আমরা সবসময় প্রত্যাশা করি ভারত বাংলাদেশের মানুষের পাশে দাঁড়াবে। কোনো একটা বিশেষ রাজনৈতিক দল আওয়ামী লীগের পাশে দাঁড়িয়ে বাংলাদেশের গণতন্ত্রকে ক্ষতিগ্রস্ত করবে না, একথা আমরা প্রত্যাশা করি। বিএনপি মহাসচিব বলেন, 'আমরা আশা করি ভারত অতীতে যে ভুল করেছে এবং বাংলাদেশের মানুষের সঙ্গে যে বিচ্ছিন্নতা তৈরি করেছে এটাকে তারা কাটিয়ে ওঠার চেষ্টা করবে এবং তাদের এখন যে কার্যকলাপ যারা বাংলাদেশের বিরুদ্ধে করছে সেই কার্যকলাপ তারা বন্ধ করবে।' তিনি বলেন, 'আজকে অত্যন্ত দুর্ভাগ্যজনকভাবে একটা ভ্রান্ত ধারণা ছড়িয়ে দিচ্ছে কিছু মানুষ। বলা হচ্ছে বাংলাদেশে সংখ্যালঘুদের ওপর নির্যাতন হচ্ছে। এই ফেইক একটা তথ্য, এই ডিজইনফরমেশন ছড়িয়ে দিয়ে আবারও একটা সমস্যা তৈরির পাঁয়তারা চলছে। এটি নিয়ে নেতাকর্মীদের সজাগ থাকতে বলেন তিনি। এসময় আগরতলায় ডেপুটি হাইকমিশনে হামলার ঘটনায় প্রতিবাদ জানিয়ে তিনি বলেন, আগরতলায় ডেপুটি হাইকমিশন অফিস আক্রান্ত হয়েছে, কলকাতায় ডেপুটি হাইকমিশন অফিস আক্রান্ত হয়েছে এবং মিছিল নিয়ে সিলেট দিয়ে ঢোকার চেষ্টা করেছে, বেনাপোল দিয়ে ঢোকার চেষ্টা করেছে। এটা কোন ধরনের বন্ধুত্ব? এটা কোন ধরনের প্রতিবেশীর কাজ? তিনি বলেন, 'আমরা এই সমাবেশ থেকে ভারতবর্ষকে অনুরোধ জানাতে চাই আপনি অনেক বড় দেশ কিন্তু তাই বলে বাংলাদেশ, যে দেশ যুদ্ধ করে স্বাধীন হয়েছে, যে সংগ্রাম করে লড়াই করে যে গণতন্ত্রকে আদায় করে নিয়েছে যারা বুকের রক্ত দিয়ে তাদের অধিকারকে আদায় করেছে তাদেরকে খাটো করে দেখে বাংলাদেশের বিরুদ্ধে দাঁড়াবেন না। কারণ বাংলাদেশের মানুষ কখনোই তা মেনে নেবে না।' মির্জা ফখরুল এসময় রাজনৈতিক দলগুলোর কাছে আহ্বান জানিয়ে বলেন, এই বিষয়ে সবাই আমরা একজোট থাকব, একমত থাকব। এবং আমরা অবশ্যই আমাদের স্বাধীনতা সার্বভৌমত্বকে অক্ষুণ্ন রাখার জন্য আমরা সবাই একজোট হয়ে আবার ঐক্যবদ্ধ হব। তিনি বলেন, শেখ হাসিনা গত ১৫ বছরে ২০ হাজার তরুণকে হত্যা করেছেন, আমাদের ইলিয়াস আলিসহ ৭০০ মানুষকে গুম করে দিয়েছেন। খালেদা জিয়াকে বছরের পর বছর কারাগারে আটকে রেখেছেন। তারেক জিয়াকে মিথ্যা মামলা দিয়ে নির্বাসিত করেছেন। বিগত আওয়ামী লীগ সরকারের দুর্নীতির কথা জানিয়ে তিনি বলেন, 'বিগত সরকার বাংলাদেশের যে ক্ষতি করে দিয়েছে, যে শ্বেতপত্র প্রকাশ হয়েছে তাতে বছরে ১৬ বিলিয়ন ডলার করে পাচার করা হয়েছে বলে উঠে এসেছে। চিন্তা করা যায়! একটা দেশের সরকার তারা দেশের সমস্ত সম্পদ লুট করে বিদেশে পাচার করে দিয়েছে। গত ১৫ বছরে বাংলাদেশের অর্থনীতি ফোকলা করে দিয়েছে। কিছু নাই ভেতরে। তারা রাজনৈতিক যে কাঠামোকে ধ্বংস করে দিয়েছে। নির্বাচন ব্যবস্থাকে ধ্বংস করে দিয়েছিল। তিনটি নির্বাচনকে বেআইনিভাবে দখল করে নিয়েছিল।' ফখরুল বলেন, 'এই সরকারের দায়িত্বটা কী? এই সরকারের দায়িত্ব হচ্ছে, খুব দ্রুত এই জঞ্জালগুলোকে সাফ করে দিয়ে যত দ্রুত সম্ভব নির্বাচন দেওয়া।' 'আমরা মনে করি নির্বাচনী ব্যবস্থা, প্রশাসন, বিচারব্যবস্থা ও অর্থনৈতিক ন্যূনতম যে সংস্কারগুলো প্রয়োজন, সেগুলো করে দ্রুত নির্বাচনের দিকে যাওয়া উচিত। কারণ, আমরা আগেও বলেছি নির্বাচন যত দেরি হবে, সমস্যা তত বাড়তে থাকবে। তত বেশি গণতন্ত্রবিরোধী শক্তি মাথা উঁচু করে দাঁড়াবে।' নেতাকর্মীদের উদ্দেশে বলেন, 'কোনো হঠকারিতা নয়, বিশৃঙ্খলা নয় অত্যন্ত সতর্কতার সঙ্গে আমাদের সামনের দিকে পা দিতে হবে। আমাদের মধ্যে অনেকে আছেন যারা উগ্রতা করেন, হঠকারিতা করেন। এগুলো কখনোই আমাদের সঠিক লক্ষ্যের দিকে নিয়ে যাবে না।' গোটা পৃথিবী এখন ভিন্নরকম, আবার অস্থির হয়ে উঠেছে জানিয়ে মির্জা ফখরুল বলেন, 'আজকে ভূরাজনীতি, ভারত উপমহাদেশের যে রাজনীতি সব নিয়েই আমাদের সামনের দিকে এগিয়ে যেতে হবে সতর্কতার সঙ্গে।'\"}\n",
            "- {'title': 'কন্ডিশনের কারণে বড় অর্জন মনে করছেন তাসকিনরা', 'news_link': 'https://bangla.thedailystar.net/sports/cricket/news-633996', 'news_text': \"ওয়েস্ট ইন্ডিজের মাঠে টেস্টে এর আগে দুই জয় ছিলো বাংলাদেশের। ১৫ বছর আগে ২০০৯ সালে সেই দুই জয় এসেছিলো খেলোয়াড় বিদ্রোহের কারণে খর্ব-শক্তির দল নিয়ে খেলা দলের বিপক্ষে। এরপর আরও তিনবার সফরে গিয়ে ফিরতে হয়েছিলো বিশাল সব হার নিয়ে। এবারও প্রথম টেস্টে বড় হারে একই পরিণতির শঙ্কা ছিলো। কিন্তু দারুণভাবে ঘুরে দাঁড়িয়ে বাংলাদেশ ফিরছে সিরিজ সমতায় শেষ করার স্বস্তি নিয়ে। এবারের জয়টাকে তাই নানান কারণে অনেক বড় অর্জন মনে হচ্ছে সিরিজ সেরা তাসকিন আহমেদ ও ম্যাচ সেরা তাইজুল ইসলামের। জ্যামাইকায় প্রথম ইনিংসে অল্প রানে গুটিয়েও ঘুরে দাঁড়িয়ে ১০১ রানের জয় বাংলাদেশ দলকে দিচ্ছে দম ফেলার ফুরসত। কারণ আগের টেস্টে ২০১ রানে হার, তার আগে ঘরের মাঠে দক্ষিণ আফ্রিকার কাছে হোয়াইটওয়াশ, ভারতে গিয়েও একই পরিণতি। অগাস্টে পাকিস্তানকে হোয়াইটওয়াশ করার স্মৃতি হয়ে পড়েছিলো ফিকে। হতাশার চক্র থেকে বেরুনোর তাগিদ ছিলো প্রবল। কঠিন কন্ডিশনের সঙ্গে চলমান চাপ মিলিয়ে এই জয় তাই বিশেষ। দুই টেস্টের সিরিজে সর্বোচ্চ ১১ উইকেট নেওয়া তাসকিন বললেন তেমনটাই, 'এটা অনেক বড় অর্জন। কারণ কঠিন কন্ডিশনে জিতেছে। ওদের কন্ডিশনে অনেক বড় বড় দলও সংগ্রাম করে। আমরা কঠিন সময় পার করছিলাম। পাকিস্তানের বিপক্ষে সিরিজ জেতার পর আমরা কয়েকটা সিরিজ হারাতে মানসিকভাবে বিপর্যস্ত ছিলাম। শক্তভাবে ফিরে এসেছি। দুইটা ম্যাচে সেরাটা দিয়ে চেষ্টা করেছি, সিরিজ সেরা হয়েছি। আমি আমার বাহু (চোটগ্রস্থ) নিয়ে টেস্টে আসার চেষ্টা করছিলাম। এখন আগের চেয়ে ভালো আছি। আশা করছি এমন অনেক অর্জন ধরা দেবে।' দুই ইনিংসেই ম্যাচের গুরুত্বপূর্ণ ধাপে তাসকিন বাংলাদেশকে রাখেন ম্যাচে। ঠিক একই কাজ তাইজুল করলেন দ্বিতীয় টেস্টে। ২৮৭ রানের লক্ষ্যে খেলতে থাকা ক্যারিবিয়ানদের ১৮৫ রানে গুটিয়ে দিতে ৫০ রানে ৫ শিকার ধরলেন বাঁহাতি স্পিনার। ম্যাচ সেরা হয়ে বললেন ভেতরে থাকা তাড়নাবোধ থেকে জয় পেয়েছেন তারা,\\xa0 'বাংলাদেশ দলের জন্য এটা অনেক বড় পাওয়া। এখানে অনেক তরুণ খেলোয়াড় ছিলো, কিছু খেলোয়াড় ছিলো হয়তবা ৮-১০ বছর ধরে খেলছে। সবাই ভালো একটা সমন্বয় ছিলো। সবার মধ্যে ভালো একটা তাড়না ছিলো ম্যাচ জিতব, সবাই যে চেষ্টা করেছে সেটা অসাধারণ ও অতুলনীয়।' ' চতুর্থ ইনিংসে যখন বোলিং করতে এসেছি, আমার ওপরে দলের একটা বড় চাওয়া ছিল। আমার বোলিং নিয়ে আমি সন্তুষ্ট। দলের যে চাওয়া ছিল ওটা আলহামদুলিল্লাহ আমি পূরণ করতে পেরেছি।'\"}\n",
            "\n",
            "If you need any further assistance or more information, please feel free to ask.\n",
            "--------------------------------------------------\n",
            "Chatbot: None\n",
            "You: exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z_Gpwd2YMYi8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}